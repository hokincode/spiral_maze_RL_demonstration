{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MazeGame:\n",
    "    def __init__(self):\n",
    "        maze_layout = [[1, 1, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 0, 1],\n",
    "               [1, 0, 1, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 0],\n",
    "               [1, 1, 1, 1, 1, 1, 1]]\n",
    "        \"\"\"Initialize the maze game with a given layout.\"\"\"\n",
    "        self.layout = np.array(maze_layout)\n",
    "        self.rgb_maze = self._to_rgb()\n",
    "        self.start_pos = None\n",
    "        self.exit_pos = None\n",
    "        self.current_pos = None\n",
    "\n",
    "    def _to_rgb(self):\n",
    "        \"\"\"Convert the maze layout to an RGB image.\"\"\"\n",
    "        rgb = np.zeros((*self.layout.shape, 3))  # Create a new RGB array\n",
    "        for i in range(3):  # Copy the grayscale values into each RGB channel\n",
    "            rgb[:, :, i] = 1 - self.layout\n",
    "        return rgb\n",
    "\n",
    "    def initialPosition(self, row, col):\n",
    "        \"\"\"Set the initial position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.start_pos = (row, col)\n",
    "            self.current_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 0, 1])  # Mark initial position with blue\n",
    "        else:\n",
    "            print(\"Invalid initial position: It is on a wall.\")\n",
    "\n",
    "    def setExit(self, row, col):\n",
    "        \"\"\"Set the exit position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.exit_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 1, 0])  # Mark exit position with green\n",
    "        else:\n",
    "            print(\"Invalid exit position: It is on a wall.\")\n",
    "\n",
    "    def set_point(self, row, col, color):\n",
    "        \"\"\"Set a specific point in the maze to a given color.\"\"\"\n",
    "        self.rgb_maze[row, col] = color\n",
    "\n",
    "    def makeMove(self, direction):\n",
    "        \"\"\"Attempt to move in the specified direction.\"\"\"\n",
    "        if self.current_pos is None:\n",
    "            print(\"Initial position not set.\")\n",
    "            return 0\n",
    "\n",
    "        row, col = self.current_pos\n",
    "        if direction == \"up\":\n",
    "            new_pos = (row - 1, col)\n",
    "        elif direction == \"down\":\n",
    "            new_pos = (row + 1, col)\n",
    "        elif direction == \"left\":\n",
    "            new_pos = (row, col - 1)\n",
    "        elif direction == \"right\":\n",
    "            new_pos = (row, col + 1)\n",
    "        else:\n",
    "            print(\"Invalid direction.\")\n",
    "            return 0\n",
    "\n",
    "        if self.isValidMove(new_pos):\n",
    "            self.current_pos = new_pos\n",
    "            self.set_point(*self.start_pos, [1 - self.layout[self.start_pos]])  # Reset start position color\n",
    "            self.start_pos = new_pos\n",
    "            self.set_point(*new_pos, [0, 0, 1])  # Mark new position with blue\n",
    "            if self.current_pos == self.exit_pos:\n",
    "                # print(\"Success! You've found the way out.\")\n",
    "                return 1\n",
    "        # else:\n",
    "        #    print(\"Invalid move: Can't move through walls or out of bounds.\")\n",
    "\n",
    "    def isValidMove(self, pos):\n",
    "        \"\"\"Check if a move is valid (within bounds and not through walls).\"\"\"\n",
    "        row, col = pos\n",
    "        if 0 <= row < self.layout.shape[0] and 0 <= col < self.layout.shape[1]:\n",
    "            return self.layout[row, col] == 0\n",
    "        return False\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Plot the maze.\"\"\"\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(self.rgb_maze, interpolation='nearest')\n",
    "        plt.xticks([]), plt.yticks([])  # Hide axes ticks\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 700x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of MazeGame\n",
    "maze_game = MazeGame()\n",
    "\n",
    "# Set the initial and exit positions\n",
    "maze_game.initialPosition(3, 3)\n",
    "maze_game.setExit(5, 6)\n",
    "maze_game.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# This is the helper function for determining if there is a wall.\n",
    "# For example, if the probability is given as 0.9, this means that\n",
    "# there is 90% that there is a wall.\n",
    "# With this helper function, we have .9 likelihood, to return 1\n",
    "# which indicate there is a wall.\n",
    "\n",
    "def choose_with_probability(s):\n",
    "    return 1 if random.random() < s else 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ModelBasedAgent:\n",
    "    def __init__(self, game, gamma=0.99):\n",
    "        self.game = game\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.internal_model = np.zeros(game.layout.shape)\n",
    "        self.values = np.zeros(game.layout.shape)\n",
    "        self.epsilon = 0.8\n",
    "        self.policy = np.full(game.layout.shape, 'up', dtype=object)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    # Bellman optimization to find the optimal action trajectory.\n",
    "    def value_iteration_of_bellman_update(self, iterations=100):\n",
    "        self.policy = np.full(self.game.layout.shape, 'up', dtype=object)\n",
    "        self.values = np.zeros(self.game.layout.shape)\n",
    "        for _ in range(iterations):\n",
    "            new_values = np.copy(self.values)\n",
    "            for row in range(self.internal_model.shape[0]):\n",
    "                for col in range(self.internal_model.shape[1]):\n",
    "                    state_value = []\n",
    "                    for action in self.actions:\n",
    "                        new_pos = self.predict_move((row, col), action)\n",
    "                        reward = self.predict_reward(new_pos)\n",
    "                        state_value.append(reward + self.gamma * self.values[new_pos])\n",
    "                    best_action_value = max(state_value)\n",
    "                    new_values[row, col] = best_action_value\n",
    "                    self.policy[row, col] = self.actions[state_value.index(best_action_value)]\n",
    "            self.values = new_values\n",
    "\n",
    "    def move_outcome_if_there_is_no_wall(self, pos, action):\n",
    "        new_pos = pos\n",
    "        if action == \"up\":\n",
    "            new_pos = (max(0, pos[0]-1), pos[1])\n",
    "        elif action == \"down\":\n",
    "            new_pos = (min(self.game.layout.shape[0]-1, pos[0]+1), pos[1])\n",
    "        elif action == \"left\":\n",
    "            new_pos = (pos[0], max(0, pos[1]-1))\n",
    "        elif action == \"right\":\n",
    "            new_pos = (pos[0], min(self.game.layout.shape[1]-1, pos[1]+1))\n",
    "        return new_pos\n",
    "\n",
    "    def predict_move(self, pos, action):\n",
    "        new_pos = pos\n",
    "        if action == \"up\":\n",
    "            new_pos = (max(0, pos[0]-1), pos[1])\n",
    "        elif action == \"down\":\n",
    "            new_pos = (min(self.game.layout.shape[0]-1, pos[0]+1), pos[1])\n",
    "        elif action == \"left\":\n",
    "            new_pos = (pos[0], max(0, pos[1]-1))\n",
    "        elif action == \"right\":\n",
    "            new_pos = (pos[0], min(self.game.layout.shape[1]-1, pos[1]+1))\n",
    "\n",
    "        # return current position if there is a wall; which is indicated with 1\n",
    "        return pos if choose_with_probability(self.internal_model[new_pos]) else new_pos\n",
    "\n",
    "    def predict_reward(self, new_pos):\n",
    "        if new_pos == self.game.exit_pos:\n",
    "            return .1  # reward for reaching the exit\n",
    "        else:\n",
    "            return 0 # no penalty for each move to encourage shortest path\n",
    "\n",
    "    def latent_learning_of_internal_model(self):\n",
    "        current_pos = self.game.current_pos\n",
    "        steps = 0\n",
    "        while current_pos != self.game.exit_pos:\n",
    "            old_position= self.game.current_pos\n",
    "\n",
    "            # explore versus exploit\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                action = random.choice(self.actions)  # Explore\n",
    "            else:\n",
    "                action = self.policy[current_pos]\n",
    "\n",
    "            self.game.makeMove(action)\n",
    "            current_pos = self.game.current_pos\n",
    "\n",
    "            if old_position == current_pos:\n",
    "                there_is_a_wall_at = self.move_outcome_if_there_is_no_wall(current_pos, action)\n",
    "                # update internal model\n",
    "                self.internal_model[there_is_a_wall_at] = self.internal_model[there_is_a_wall_at] + 0.01\n",
    "            steps += 1\n",
    "            if steps > 5:  # each train wandering has 5 steps\n",
    "                break\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            self.game.initialPosition(3, 3)  # Reset to starting position\n",
    "            self.latent_learning_of_internal_model()\n",
    "            self.value_iteration_of_bellman_update()\n",
    "\n",
    "    def run_policy(self):\n",
    "        print(\"Running policy...\")\n",
    "        print('Action policy', self.policy)\n",
    "        print('Value,', self.values)\n",
    "        print('Internal Model,', self.internal_model)\n",
    "        current_pos = self.game.current_pos\n",
    "        steps = 0\n",
    "        while current_pos != self.game.exit_pos:\n",
    "            action = self.policy[current_pos]\n",
    "            print(f\"Step {steps}: at {current_pos} taking action {action}\")\n",
    "            self.game.makeMove(action)\n",
    "            current_pos = self.game.current_pos\n",
    "            steps += 1\n",
    "            if steps > 50:  # Just in case something goes wrong\n",
    "                print(\"Stopping after 50 steps.\")\n",
    "                break\n",
    "\n",
    "    def update_game(self, new_game):\n",
    "        self.game = new_game"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m new_agent \u001B[38;5;241m=\u001B[39m ModelBasedAgent(maze_game)\n\u001B[0;32m----> 2\u001B[0m new_agent\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;241m100000\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 88\u001B[0m, in \u001B[0;36mModelBasedAgent.train\u001B[0;34m(self, episodes)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgame\u001B[38;5;241m.\u001B[39minitialPosition(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m)  \u001B[38;5;66;03m# Reset to starting position\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent_learning_of_internal_model()\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_iteration_of_bellman_update()\n",
      "Cell \u001B[0;32mIn[5], line 21\u001B[0m, in \u001B[0;36mModelBasedAgent.value_iteration_of_bellman_update\u001B[0;34m(self, iterations)\u001B[0m\n\u001B[1;32m     19\u001B[0m state_value \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactions:\n\u001B[0;32m---> 21\u001B[0m     new_pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict_move((row, col), action)\n\u001B[1;32m     22\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict_reward(new_pos)\n\u001B[1;32m     23\u001B[0m     state_value\u001B[38;5;241m.\u001B[39mappend(reward \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues[new_pos])\n",
      "Cell \u001B[0;32mIn[5], line 53\u001B[0m, in \u001B[0;36mModelBasedAgent.predict_move\u001B[0;34m(self, pos, action)\u001B[0m\n\u001B[1;32m     50\u001B[0m     new_pos \u001B[38;5;241m=\u001B[39m (pos[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgame\u001B[38;5;241m.\u001B[39mlayout\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, pos[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# return current position if there is a wall; which is indicated with 1\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pos \u001B[38;5;28;01mif\u001B[39;00m choose_with_probability(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minternal_model[new_pos]) \u001B[38;5;28;01melse\u001B[39;00m new_pos\n",
      "Cell \u001B[0;32mIn[4], line 8\u001B[0m, in \u001B[0;36mchoose_with_probability\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchoose_with_probability\u001B[39m(s):\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m random\u001B[38;5;241m.\u001B[39mrandom() \u001B[38;5;241m<\u001B[39m s \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "new_agent = ModelBasedAgent(maze_game)\n",
    "new_agent.train(10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy...\n",
      "Action policy [['down' 'down' 'down' 'down' 'down' 'down' 'up']\n",
      " ['down' 'down' 'down' 'left' 'left' 'left' 'left']\n",
      " ['down' 'down' 'left' 'left' 'up' 'up' 'left']\n",
      " ['down' 'down' 'down' 'right' 'right' 'up' 'down']\n",
      " ['down' 'down' 'down' 'down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'right' 'right' 'right' 'right' 'right']\n",
      " ['up' 'up' 'up' 'up' 'up' 'up' 'up']]\n",
      "Value, [[4.29042033 4.38177205 4.29042033 4.19998212 4.1104483  4.02180981\n",
      "  0.        ]\n",
      " [4.38177205 4.47404652 4.38177205 4.29042033 4.19998212 4.1104483\n",
      "  4.02180981]\n",
      " [4.47404652 4.56725306 4.47404652 4.38177205 4.1104483  4.02180981\n",
      "  3.93405771]\n",
      " [4.56725306 4.66140107 4.75650008 3.76117729 3.84718313 3.93405771\n",
      "  5.14659958]\n",
      " [4.66140107 4.75650008 4.85255968 4.94958958 5.04759958 5.14659958\n",
      "  5.24659958]\n",
      " [4.75650008 4.85255968 4.94958958 5.04759958 5.14659958 5.24659958\n",
      "  5.24659958]\n",
      " [4.66140107 4.75650008 4.85255968 4.94958958 5.04759958 5.14659958\n",
      "  5.24659958]]\n",
      "Internal Model, [[  0.     0.     0.     0.     1.08   9.29   0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     8.98]\n",
      " [  0.     0.     0.   317.16 231.52   0.    36.3 ]\n",
      " [  0.     0.   316.23   0.     0.     0.    94.25]\n",
      " [  0.     0.     0.   314.63 197.75  95.74   0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]]\n",
      "Step 0: at (3, 4) taking action right\n",
      "Step 1: at (3, 5) taking action up\n",
      "Step 2: at (2, 5) taking action up\n",
      "Step 3: at (1, 5) taking action left\n",
      "Step 4: at (1, 4) taking action left\n",
      "Step 5: at (1, 3) taking action left\n",
      "Step 6: at (1, 2) taking action down\n",
      "Step 7: at (1, 2) taking action down\n",
      "Step 8: at (1, 2) taking action down\n",
      "Step 9: at (1, 2) taking action down\n",
      "Step 10: at (1, 2) taking action down\n",
      "Step 11: at (1, 2) taking action down\n",
      "Step 12: at (1, 2) taking action down\n",
      "Step 13: at (1, 2) taking action down\n",
      "Step 14: at (1, 2) taking action down\n",
      "Step 15: at (1, 2) taking action down\n",
      "Step 16: at (1, 2) taking action down\n",
      "Step 17: at (1, 2) taking action down\n",
      "Step 18: at (1, 2) taking action down\n",
      "Step 19: at (1, 2) taking action down\n",
      "Step 20: at (1, 2) taking action down\n",
      "Step 21: at (1, 2) taking action down\n",
      "Step 22: at (1, 2) taking action down\n",
      "Step 23: at (1, 2) taking action down\n",
      "Step 24: at (1, 2) taking action down\n",
      "Step 25: at (1, 2) taking action down\n",
      "Step 26: at (1, 2) taking action down\n",
      "Step 27: at (1, 2) taking action down\n",
      "Step 28: at (1, 2) taking action down\n",
      "Step 29: at (1, 2) taking action down\n",
      "Step 30: at (1, 2) taking action down\n",
      "Step 31: at (1, 2) taking action down\n",
      "Step 32: at (1, 2) taking action down\n",
      "Step 33: at (1, 2) taking action down\n",
      "Step 34: at (1, 2) taking action down\n",
      "Step 35: at (1, 2) taking action down\n",
      "Step 36: at (1, 2) taking action down\n",
      "Step 37: at (1, 2) taking action down\n",
      "Step 38: at (1, 2) taking action down\n",
      "Step 39: at (1, 2) taking action down\n",
      "Step 40: at (1, 2) taking action down\n",
      "Step 41: at (1, 2) taking action down\n",
      "Step 42: at (1, 2) taking action down\n",
      "Step 43: at (1, 2) taking action down\n",
      "Step 44: at (1, 2) taking action down\n",
      "Step 45: at (1, 2) taking action down\n",
      "Step 46: at (1, 2) taking action down\n",
      "Step 47: at (1, 2) taking action down\n",
      "Step 48: at (1, 2) taking action down\n",
      "Step 49: at (1, 2) taking action down\n",
      "Step 50: at (1, 2) taking action down\n",
      "Stopping after 50 steps.\n"
     ]
    }
   ],
   "source": [
    "new_agent.run_policy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
