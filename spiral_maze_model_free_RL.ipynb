{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MazeGame:\n",
    "    def __init__(self):\n",
    "        maze_layout = [[1, 1, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 0, 1],\n",
    "               [1, 0, 1, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 0],\n",
    "               [1, 1, 1, 1, 1, 1, 1]]\n",
    "        \"\"\"Initialize the maze game with a given layout.\"\"\"\n",
    "        self.layout = np.array(maze_layout)\n",
    "        self.rgb_maze = self._to_rgb()\n",
    "        self.start_pos = None\n",
    "        self.exit_pos = None\n",
    "        self.current_pos = None\n",
    "\n",
    "    def _to_rgb(self):\n",
    "        \"\"\"Convert the maze layout to an RGB image.\"\"\"\n",
    "        rgb = np.zeros((*self.layout.shape, 3))  # Create a new RGB array\n",
    "        for i in range(3):  # Copy the grayscale values into each RGB channel\n",
    "            rgb[:, :, i] = 1 - self.layout\n",
    "        return rgb\n",
    "\n",
    "    def initialPosition(self, row, col):\n",
    "        \"\"\"Set the initial position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.start_pos = (row, col)\n",
    "            self.current_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 0, 1])  # Mark initial position with blue\n",
    "        else:\n",
    "            print(\"Invalid initial position: It is on a wall.\")\n",
    "\n",
    "    def setExit(self, row, col):\n",
    "        \"\"\"Set the exit position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.exit_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 1, 0])  # Mark exit position with green\n",
    "        else:\n",
    "            print(\"Invalid exit position: It is on a wall.\")\n",
    "\n",
    "    def set_point(self, row, col, color):\n",
    "        \"\"\"Set a specific point in the maze to a given color.\"\"\"\n",
    "        self.rgb_maze[row, col] = color\n",
    "\n",
    "    def makeMove(self, direction):\n",
    "        \"\"\"Attempt to move in the specified direction.\"\"\"\n",
    "        if self.current_pos is None:\n",
    "            print(\"Initial position not set.\")\n",
    "            return 0\n",
    "\n",
    "        row, col = self.current_pos\n",
    "        if direction == \"up\":\n",
    "            new_pos = (row - 1, col)\n",
    "        elif direction == \"down\":\n",
    "            new_pos = (row + 1, col)\n",
    "        elif direction == \"left\":\n",
    "            new_pos = (row, col - 1)\n",
    "        elif direction == \"right\":\n",
    "            new_pos = (row, col + 1)\n",
    "        else:\n",
    "            print(\"Invalid direction.\")\n",
    "            return 0\n",
    "\n",
    "        if self.isValidMove(new_pos):\n",
    "            self.current_pos = new_pos\n",
    "            self.set_point(*self.start_pos, [1 - self.layout[self.start_pos]])  # Reset start position color\n",
    "            self.start_pos = new_pos\n",
    "            self.set_point(*new_pos, [0, 0, 1])  # Mark new position with blue\n",
    "            if self.current_pos == self.exit_pos:\n",
    "                # print(\"Success! You've found the way out.\")\n",
    "                return 1\n",
    "        # else:\n",
    "        #    print(\"Invalid move: Can't move through walls or out of bounds.\")\n",
    "\n",
    "    def isValidMove(self, pos):\n",
    "        \"\"\"Check if a move is valid (within bounds and not through walls).\"\"\"\n",
    "        row, col = pos\n",
    "        if 0 <= row < self.layout.shape[0] and 0 <= col < self.layout.shape[1]:\n",
    "            return self.layout[row, col] == 0\n",
    "        return False\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Plot the maze.\"\"\"\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(self.rgb_maze, interpolation='nearest')\n",
    "        plt.xticks([]), plt.yticks([])  # Hide axes ticks\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 700x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of MazeGame\n",
    "maze_game = MazeGame()\n",
    "\n",
    "# Set the initial and exit positions\n",
    "maze_game.initialPosition(3, 3)\n",
    "maze_game.setExit(5, 6)\n",
    "maze_game.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model-free RL agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, game, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.game = game\n",
    "        self.q_table = np.zeros((game.layout.size, 4))  # 4 actions: up, down, left, right\n",
    "        # print('At initialization, the Q table is given as,', self.q_table)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Converts a 2D state to a 1D index for the Q-table.\"\"\"\n",
    "        # print('Given state at,', state)\n",
    "        # print('The index is given as,', state[0] * self.game.layout.shape[1] + state[1])\n",
    "        return state[0] * self.game.layout.shape[1] + state[1]\n",
    "\n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"Choose action based on ε-greedy policy.\"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)  # Explore\n",
    "        else:\n",
    "            # print('exploit action at,', np.argmax(self.q_table[state_index]))\n",
    "            return self.actions[np.argmax(self.q_table[state_index])]  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value using the Q-learning formula.\"\"\"\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state)\n",
    "        action_index = self.actions.index(action)\n",
    "        next_max = np.max(self.q_table[next_state_index])\n",
    "\n",
    "        # Q-learning formula\n",
    "        self.q_table[state_index, action_index] = self.q_table[state_index, action_index] + \\\n",
    "            self.alpha * (reward + self.gamma * next_max - self.q_table[state_index, action_index])\n",
    "        # print('Update Q table,', self.q_table)\n",
    "\n",
    "    def train(self, episodes=1000, print_out_internal_variable=False):\n",
    "        for episode in range(episodes):\n",
    "            self.game.initialPosition(3, 3)  # Reset to starting position\n",
    "            state = self.game.current_pos\n",
    "            steps = 0\n",
    "            success = 0\n",
    "            while not success and steps < 50:\n",
    "                # Select and perform an action\n",
    "                action = self.choose_action(self.state_to_index(state))\n",
    "                old_state = state\n",
    "                success = self.game.makeMove(action)\n",
    "                state = self.game.current_pos\n",
    "\n",
    "                # Extremely important note here: There are generally two ways that we could design out Q learner here\n",
    "                # First, we could use memory reply and only update the Q table if the agent has reached the exit at\n",
    "                # the end. In this case, the actions of the agent is imagined as all the possible action trajectories\n",
    "                # of the agent. I personally believe that this supposedly is the correct way to build the agent.\n",
    "                # However, there is another way to build the agent. That we encode each pixel as a possible state.\n",
    "                # Then, each state has 4 possible actions. We build a 4 by 49 matrix. An action would lead to a\n",
    "                # punishment, if the agent does not move. That means, moving towards a wall would lead to negative\n",
    "                # rewards. In this way, we could deal away with the high-dimensionality of the action trajectory space.\n",
    "                # However, technically, we don't have a sparse reward signal for our Q learning agent. Instead,\n",
    "                # we have a very dense rewarded-punished Q learning agent.\n",
    "                #\n",
    "                # Nevertheless, given this really hacked trick, the Q learning agent still takes many episodes to\n",
    "                # finally actually learned how to navigate out of the maze.\n",
    "\n",
    "                reward = 1 if success else -0.5  # Simple reward: -0.5 for each step, +1 for reaching the goal\n",
    "                # Learn from the transition\n",
    "                self.learn(old_state, action, reward, state)\n",
    "                steps += 1\n",
    "            if print_out_internal_variable:\n",
    "                print(f\"Episode {episode}: the agent took {steps} steps.\")\n",
    "                print('Current Q table,', self.q_table)\n",
    "                print('The agent reaches the exit or not?', success)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: the agent took 50 steps.\n",
      "Current Q table, [[ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [-0.05     -0.05     -0.05     -0.05    ]\n",
      " [-0.05     -0.05     -0.05     -0.05    ]\n",
      " [-0.05     -0.05     -0.05     -0.05    ]\n",
      " [-0.05     -0.05     -0.05     -0.05    ]\n",
      " [-0.09995  -0.05     -0.05     -0.05    ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [-0.05      0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [-0.095    -0.05     -0.05     -0.095   ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [-0.09995  -0.09995  -0.09995  -0.14045 ]\n",
      " [-0.09995  -0.09995  -0.104405 -0.095   ]\n",
      " [-0.095    -0.05     -0.05495  -0.05    ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]]\n",
      "The agent reaches the exit or not? None\n",
      "Episode 1: the agent took 50 steps.\n",
      "Current Q table, [[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.05       -0.05       -0.05       -0.05      ]\n",
      " [-0.05       -0.05       -0.05       -0.05      ]\n",
      " [-0.05       -0.05       -0.05       -0.05      ]\n",
      " [-0.05       -0.05       -0.05       -0.05      ]\n",
      " [-0.09995    -0.104405   -0.05       -0.05      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.05        0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.181355   -0.1592407  -0.14985005 -0.144905  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.34402645 -0.299251   -0.299251   -0.32165607]\n",
      " [-0.2495005  -0.2495005  -0.26612156 -0.27597434]\n",
      " [-0.22712405 -0.1997002  -0.2166955  -0.1997002 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "The agent reaches the exit or not? None\n",
      "Episode 2: the agent took 50 steps.\n",
      "Current Q table, [[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.095      -0.09995    -0.09995   ]\n",
      " [-0.09995    -0.09995    -0.09995    -0.09995   ]\n",
      " [-0.09995    -0.09995    -0.09995    -0.09995   ]\n",
      " [-0.09995    -0.09995    -0.09995    -0.09995   ]\n",
      " [-0.14985005 -0.104405   -0.09995    -0.09995   ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.05       -0.05       -0.05      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.2181695  -0.21801718 -0.1997002  -0.1947601 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.05        0.          0.         -0.05      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.39368242 -0.3986028  -0.3986028  -0.40509337]\n",
      " [-0.34895175 -0.34895175 -0.32356802 -0.35881778]\n",
      " [-0.30983566 -0.299251   -0.2746518  -0.299251  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "The agent reaches the exit or not? None\n",
      "Episode 3: the agent took 50 steps.\n",
      "Current Q table, [[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.14045    -0.09995    -0.09995   ]\n",
      " [-0.14985005 -0.14985005 -0.14936    -0.09995   ]\n",
      " [-0.14985005 -0.14985005 -0.14985005 -0.09995   ]\n",
      " [-0.14985005 -0.14985005 -0.14985005 -0.09995   ]\n",
      " [-0.14985005 -0.104405   -0.14985005 -0.09995   ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.14045    -0.09995    -0.14936   ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.2562476  -0.27584131 -0.2495005  -0.28987912]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.095      -0.05       -0.05      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.49284545 -0.49775599 -0.49775599 -0.52731326]\n",
      " [-0.44820419 -0.44820419 -0.44095934 -0.48023096]\n",
      " [-0.38491879 -0.34895175 -0.39531432 -0.34895175]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.05        0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "The agent reaches the exit or not? None\n",
      "Episode 4: the agent took 50 steps.\n",
      "Current Q table, [[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.14045    -0.09995    -0.09995   ]\n",
      " [-0.14985005 -0.14985005 -0.14936    -0.09995   ]\n",
      " [-0.14985005 -0.14985005 -0.14985005 -0.15527923]\n",
      " [-0.1997002  -0.1997002  -0.1947601  -0.20908146]\n",
      " [-0.2495005  -0.23377136 -0.24011924 -0.2495005 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.14045    -0.09995    -0.14936   ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.36295238 -0.33771886 -0.34895175 -0.33958924]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.09995    -0.095      -0.05       -0.05      ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.63680779 -0.64611426 -0.64611426 -0.64636555]\n",
      " [-0.54725823 -0.54654171 -0.55957507 -0.5496249 ]\n",
      " [-0.45771275 -0.44820419 -0.45498972 -0.49507539]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.05        0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "The agent reaches the exit or not? None\n"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(maze_game)\n",
    "agent.train(5, True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_agent(agent, game, max_steps=50):\n",
    "    game.initialPosition(3, 3)  # Reset to starting position\n",
    "    state = game.current_pos\n",
    "    steps = 0\n",
    "    success = False\n",
    "    # Display the initial state of the maze\n",
    "    # game.plot()\n",
    "    print(f\"Starting test from position: {state}\")\n",
    "    while not success and steps < max_steps:\n",
    "        state_index = agent.state_to_index(state)\n",
    "        action = agent.actions[np.argmax(agent.q_table[state_index])]  # Choose best action\n",
    "        success = game.makeMove(action)  # Perform the action\n",
    "        state = game.current_pos  # Update state\n",
    "        steps += 1\n",
    "        print(f\"Step {steps}: Moved {action}, New position: {state}\")\n",
    "    if success:\n",
    "        print(\"Success! The agent found the way out.\")\n",
    "    else:\n",
    "        print(\"Failed to find the way out within the maximum allowed steps.\")\n",
    "\n",
    "    # Display the final state of the maze\n",
    "    game.plot()\n",
    "# Now, let's test the trained agent.\n",
    "test_agent(agent, maze_game)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
